<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Keith A. Lewis" />
  <title>Linear Algebra</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="math.css" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans&family=STIX+Two+Text&display=swap" rel="stylesheet">
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: true
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Linear Algebra</h1>
<p class="author">Keith A. Lewis</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#vector-space">Vector Space</a></li>
<li><a href="#independence">Independence</a>
<ul>
<li><a href="#geometric-algebra">Geometric Algebra</a></li>
</ul></li>
<li><a href="#subspace">Subspace</a></li>
<li><a href="#linear-operators">Linear Operators</a>
<ul>
<li><a href="#invariant-subspace">Invariant Subspace</a></li>
<li><a href="#eigenvectorsvalues">Eigenvectors/values</a></li>
<li><a href="#functional-calculus">Functional Calculus</a></li>
<li><a href="#jordan-canonical-form">Jordan Canonical Form</a></li>
</ul></li>
<li><a href="#dual">Dual</a></li>
</ul>
</nav>
<p>Linear algebra is the study of linear structures and functions that
preserve this. A vector space is not just a tuple of numbers, it is a
mathematical object satisfying axioms. The axioms imply every vector
space can be identified with the set of functions from a basis to the
underlying field. They are completely characterized the cardinality of
their basis. Linear operators between vector spaces also form a vector
space. They can also be completely characterized, but the story is more
complicated. By the prime number factorization theorem, every number
<span class="math inline">n\in\boldsymbol{N}</span> can be written as
<span class="math inline">n = p_1^{k_1}\cdots p_m^{k_m}</span> where
<span class="math inline">p_i</span> is prime and <span
class="math inline">k_i &gt; 0</span>. Every operator <span
class="math inline">T\colon V\to V</span> can be factored into <span
class="math inline">T_1\oplus\cdots T_m\colon V_1\oplus\cdots V_m\to
T_m\colon V_1\oplus\cdots V_m\to</span> where the spectrum <span
class="math inline">\sigma(T_i) = \{\lamda_i\}</span> is a singleton.
Every operator <span class="math inline">T</span> with <span
class="math inline">\sigma(T) = \{\lambda\}</span> can be factored into
<span class="math inline">T = J^\lambda_{k_1}\oplus\cdots
J^\lambda_{k_m}</span> where <span class="math inline">J^\lambda =
\lambda I + J</span> where <span class="math inline">J</span> is a shift
operator.</p>
<h2 id="vector-space">Vector Space</h2>
<p>A <em>vector space</em> over a <em>field</em> <span
class="math inline">\boldsymbol{F}</span> is set <span
class="math inline">V</span> with a binary operation <span
class="math inline">V\times V\to V</span>, <span
class="math inline">(v,w)\mapsto v + w</span>, and a scalar product
<span class="math inline">\boldsymbol{F}\times V\to V</span>, <span
class="math inline">(a,v)\mapsto av</span>, satisfying the distributive
law. Typically <span class="math inline">\boldsymbol{F}</span> is either
the real numbers <span class="math inline">\boldsymbol{R}</span> or
complex numbers <span class="math inline">\boldsymbol{C}</span>. The
binary addition is <em>commutative</em> (<span class="math inline">v + w
= w + v</span>), <em>associative</em> (<span class="math inline">(u + v)
+ w = u + (w + v)</span>), has an <em>identity element</em> <span
class="math inline">\boldsymbol{0}</span> (<span class="math inline">v +
\boldsymbol{0}= v</span>), and each element has an inverse (<span
class="math inline">v + (-v) = \boldsymbol{0}</span>). The scalar
product satisfies the <em>distributive laws</em> <span
class="math inline">a(v + w) = av + aw</span>, <span
class="math inline">(a + b)v = av + bv</span>, <span
class="math inline">(ab)v = a(bv)</span>, <span
class="math inline">a,b\in\boldsymbol{F}</span>, <span
class="math inline">v,w\in V</span>. We also require <span
class="math inline">1v = v</span> and <span class="math inline">av =
va</span> for <span class="math inline">a\in\boldsymbol{F}</span> and
<span class="math inline">v\in V</span>.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">v + z =
v</span> for all <span class="math inline">v\in V</span> then <span
class="math inline">z = \boldsymbol{0}</span></em>.</p>
<p>This shows the additive identity is unique.</p>
<details>
<summary>
Solution
</summary>
Taking <span class="math inline">v = \boldsymbol{0}</span>, <span
class="math inline">z = \boldsymbol{0}+ z = \boldsymbol{0}</span>.
</details>
<p><strong>Exercise</strong>. <em>If <span class="math inline">v + v =
v</span> then <span class="math inline">v =
\boldsymbol{0}</span></em>.</p>
<p><em>Hint</em>: <span class="math inline">v + (-v) =
\boldsymbol{0}</span>.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">0v =
\boldsymbol{0}</span>, <span class="math inline">a\boldsymbol{0}=
\boldsymbol{0}</span>, and <span class="math inline">(-1)v = -v</span>,
<span class="math inline">a\in\boldsymbol{F}</span>, <span
class="math inline">v\in V</span></em>.</p>
<details>
<summary>
Solution
</summary>
Note <span class="math inline">0v + 0v = (0 + 0)v = 0v</span> so <span
class="math inline">0v = \boldsymbol{0}</span>. If <span
class="math inline">a\not=0</span> then for any <span
class="math inline">v\in V</span> we have <span class="math inline">v +
a\boldsymbol{0}= aa^{-1}v + a\boldsymbol{0}= a(a^{-1}v + \boldsymbol{0})
= aa^{-1}v = v</span> so <span class="math inline">a\boldsymbol{0}=
\boldsymbol{0}</span> since the identity is unique. Since <span
class="math inline">v + (-1)v = 1v + (-1)v = (1 + (-1))v) = 0v =
\boldsymbol{0}</span> we have <span class="math inline">(-1)v =
-v</span>.
</details>
<p>If <span class="math inline">S</span> is any set then the set of all
functions from <span class="math inline">S</span> to <span
class="math inline">\boldsymbol{F}</span>, <span
class="math inline">\boldsymbol{F}^S = \{v\colon
S\to\boldsymbol{F}\}</span>, is a vector space. The addition is defined
pointwise <span class="math display">
    (v + w)(s) = v(s) + w(s)\text{ for } v, w\in\boldsymbol{F}^S
</span> as is the scalar product <span class="math display">
    (av)(s) = av(s)\text{ for } a\in\boldsymbol{F},
v\in\boldsymbol{F}^S.
</span></p>
<p>The additive identity, <span
class="math inline">\boldsymbol{0}</span>, is the function <span
class="math inline">\boldsymbol{0}(s) = 0</span> for all <span
class="math inline">s\in S</span>. Note there are two different brands
of plus sign in <span class="math inline">(v + w)(s) = v(s) +
w(s)</span>. The one on the left is the vector space addition and the
one on the right is addition in the underlying field.</p>
<p><strong>Exercise</strong>. <em>Show <span
class="math inline">\boldsymbol{F}^S</span> is a vector space</em>.</p>
<details>
<summary>
Solution
</summary>
The vector space addition is commutative since <span
class="math inline">(v + w)(s) = v(s) + w(s) = w(s) + v(s) = (w +
v)(s)</span> and the field addition is commutative. Similarly, <span
class="math inline">(u + v) + w = u + (v + w)</span> since the field
addition is associative. The <span
class="math inline">\boldsymbol{0}</span> vector satisfies <span
class="math inline">(v + \boldsymbol{0})(s) = v(s) + \boldsymbol{0}(s) =
v(s) + 0 = v(s)</span> so <span class="math inline">v + \boldsymbol{0}=
v</span>.
</details>
<p>We will see later that every vector space has this form. The
cardinality of <span class="math inline">S</span> is the
<em>dimension</em> of the vector space.</p>
<!--

__Exercise__. (Abelian group under addition) _For $u, v, w\in\RR^S$ prove_
$$
\begin{aligned}
\text{(commutative)}\quad&& v + w &= w + v \\
\text{(associative)}\quad&& (u + v) + w &= u + (w + v)  \\
\text{(identity)}\quad&& \zero + v = &\ v = v + \zero \\
\text{(inverse)}\quad&& (-v) + v = &\ \zero = v + (-v) \\
\end{aligned}
$$

_Hint_: $\RR$ is a abelian group under addition.

__Exercise__. (Scalar multiplication) _For $v\in\RR^S$ and $a,b\in\RR$ prove_
$$
\begin{aligned}
0v &= \zero \\
1v &= v \\
(ab)v &= a(bv) \\
\end{aligned}
$$

__Exercise__. (Distributive laws) _For $a,b\in\RR$ and $v,w\in\RR^S$ prove_
$$
\begin{aligned}
a(v + w) &= av + aw \\
(a + b)v &= av + bv \\
\end{aligned}
$$

<details>
<summary>Solution</summary>
We have $(a(v + w))(s) = a((v + w)(s)) = a(v(s) + w(s)) = av(s) + aw(s) = (av + aw)(s)$
and $(a + b)v(s) = av(s) + bv(s)$, $s\in S$,
by the distributive law for real numbers.
</details>
-->
<p>You are probably already familiar with the vector space <span
class="math inline">\boldsymbol{R}^n = \{(x_1,\ldots,x_n)\mid
x_i\in\boldsymbol{R}, 1\le i\le n\}</span>, where <span
class="math inline">\boldsymbol{R}</span> is the real numbers. If <span
class="math inline">S = \{1,\ldots,n\}</span> and <span
class="math inline">x\in\boldsymbol{R}^S</span> then <span
class="math inline">x(i) = x_i</span> provides a correspondance between
<span class="math inline">\boldsymbol{R}^{\{1,\ldots,n\}}</span> and
<span class="math inline">\boldsymbol{R}^n</span>. The <em>Kronecker
delta</em> is <span class="math inline">\delta_{ij} = 1</span> if <span
class="math inline">i = j</span> and <span
class="math inline">\delta_{ij} = 0</span> if <span
class="math inline">i \not= j</span>. The <em>standard basis</em> in
<span class="math inline">\boldsymbol{R}^n</span> is <span
class="math inline">\{e_j\}_{1\le j\le n}\subset\boldsymbol{R}^n</span>
where <span class="math inline">(e_j)_i = \delta_{ij}</span>, <span
class="math inline">1\le i\le n</span>.</p>
<p><strong>Exercise</strong>. <em>Show every vector <span
class="math inline">x = (x_1,\ldots,x_n)\in\boldsymbol{R}^n</span> can
be written using the standard basis as <span class="math inline">x =
\sum_j x_j e_j</span></em>.</p>
<h2 id="independence">Independence</h2>
<p>A fundamental concept in linear algebra is <em>independence</em>. A
set of vectors <span class="math inline">S\subset V</span> is
<em>independent</em> if every finite sum <span
class="math inline">\sum_{v\in S} a_v v = \boldsymbol{0}</span>, <span
class="math inline">a_v\in\boldsymbol{F}</span>, implies <span
class="math inline">a_v = 0</span> for all <span
class="math inline">v\in S</span>. A <em>basis</em> of a vector space is
a set of independent vectors that span <span
class="math inline">V</span>.</p>
<p><strong>Exercise</strong>. <em>Show every singleton <span
class="math inline">\{v\}</span> where <span
class="math inline">v\not=0</span> is independent</em>.</p>
<p><strong>Exercise</strong>. <em>Show <span
class="math inline">\{v,w\}</span>, <span
class="math inline">v,w\not=\boldsymbol{0}</span>, is (not in)dependent
if and only if <span class="math inline">v = aw</span> for some <span
class="math inline">a\in\boldsymbol{F}</span></em>.</p>
<p><strong>Exercise</strong>. <em>If <span
class="math inline">\{v,w\}</span> are independent then <span
class="math inline">\{av + bw, cv + dw\}</span> are independent if and
only if <span class="math inline">ad - bc\not=0</span></em>.</p>
<details>
<summary>
Solution
</summary>
<span class="math inline">\{av + bw, cv + dw\}</span> are dependent if
and only if <span class="math inline">av + bw = e(cw + dw)</span> for
some non-zero <span class="math inline">e\in\boldsymbol{F}</span>. This
is equivalent to <span class="math inline">a = ec</span> and <span
class="math inline">b = ed</span> so <span class="math inline">ad - bc =
ecd - edc = 0</span>. If <span class="math inline">ad - bc = 0</span>
then <span class="math inline">a/c = b/d</span> and <span
class="math inline">a = ec</span>, <span class="math inline">b =
ed</span> where <span class="math inline">e = a/c = b/d</span>.
</details>
<p><strong>Exercise</strong>. <em>Show <span
class="math inline">\{v_i\}</span> are dependent if and only if <span
class="math inline">v_j = \sum_{i\not=j} a_i v_i</span>, <span
class="math inline">a_i\in\boldsymbol{F}</span>, for some <span
class="math inline">j</span></em>.</p>
<p>In this case we say <span class="math inline">v_j</span> is in the
<em>span</em> of <span class="math inline">\{v_i\}_{i\not=j}</span>.</p>
<h3 id="geometric-algebra">Geometric Algebra</h3>
<p>Hermann Grassmann invented <em>geometric algebra</em>. Let <span
class="math inline">E</span> be the points in space and consider the
algebra they generate. Elements of the algebra have the form <span
class="math display">
a + \sum_i a_i P_i + \sum_{i,j} a_{ij}P_i P_j + \sum_{i,j,k} a_{ijk} P_i
P_j P_k + \cdots
</span> where <span class="math inline">a, a_i, a_{ij}, a_{ijk} \ldots
\in\boldsymbol{F}</span> and <span class="math inline">P_i\in E</span>.
There is only one rule: <span class="math inline">PQ = 0</span> if and
only if <span class="math inline">P = Q</span>.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">PQ =
-QP</span> for <span class="math inline">P,Q\in E</span></em>.</p>
<details>
<summary>
Solution
</summary>
<span class="math inline">0 = (P + Q)(P + Q) = PP + PQ + QP + QQ = PQ +
QP</span>.
</details>
<p>The definition of independence is similar to that for vector spaces.
The points <span class="math inline">\{P_i\}\subseteq E</span> are
<em>independent</em> if <span class="math inline">\sum_i a_i P_i =
0</span> implies <span class="math inline">a_i = 0</span> for all <span
class="math inline">i</span>. The <span class="math inline">0</span> in
the first equation is <span
class="math inline">0\in\boldsymbol{F}</span>, not the vector <span
class="math inline">\boldsymbol{0}</span>.</p>
<p><strong>Exercise</strong>. <em>Show if <span
class="math inline">P</span> and <span class="math inline">Q</span> are
independent then <span class="math inline">(aP + bQ)(cP + dQ) = (ad -
bc)PQ</span></em>.</p>
<p>Vectors arise as differences of points. The set <span
class="math inline">\{P - Q\mid P,Q\in E\}</span> is a vector space with
zero vector <span class="math inline">\boldsymbol{0}= P - P</span> for
any <span class="math inline">P\in E</span>.</p>
<p>The definition of <em>determinant</em> is greatly simplified in
Grasmann’s algebra. If <span class="math inline">[a_{ij}]</span> is a
square matrix then <span class="math display">
    \prod_i (\sum_j a_{i,j}P_j) = \det [a_{i,j}].
</span></p>
<h2 id="subspace">Subspace</h2>
<p>A subset <span class="math inline">U\subseteq V</span> is a
<em>subspace</em> of <span class="math inline">V</span> if <span
class="math inline">U</span> is also a vector space. Clearly, <span
class="math inline">\{\boldsymbol{0}\}</span> is the smallest subspace
and <span class="math inline">V</span> is the largest subspace of <span
class="math inline">V</span>.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">U</span>
and <span class="math inline">W</span> are subspaces then so are <span
class="math inline">U\cap W</span> and <span class="math inline">U +
W</span></em>.</p>
<p><em>Hint</em>: <span class="math inline">U + W = \{u + w\mid u\in U,
w\in W\}</span>.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">v\in
V</span> then <span class="math inline">\boldsymbol{F}v = \{av\mid
a\in\boldsymbol{F}\}</span> is a subspace</em>.</p>
<p>Given any set <span class="math inline">S\subset V</span> define
<span class="math inline">\operatorname{span}S</span> to be the smallest
subspace containing <span class="math inline">S</span>.</p>
<p><strong>Exercise</strong>. <em>If <span
class="math inline">S\subseteq V</span> then <span
class="math inline">\operatorname{span}S = \{\sum_{s\in S_0} a_s s\mid
a_s\in\boldsymbol{F}, S_0\subseteq S \text{ finite }\}</span></em>.</p>
<p>We need <span class="math inline">S_0</span> to be finite for the sum
to be defined when <span class="math inline">S</span> is infinite.</p>
<details>
<summary>
Solution
</summary>
Taking <span class="math inline">S_0 = \{s\}</span> and <span
class="math inline">a_s = 1</span> we see <span
class="math inline">S</span> is a subset of the right hand side. Every
term of the form <span class="math inline">\sum_{s\in S_0} a_s s</span>
must belong to <span class="math inline">\operatorname{span}S</span>.
Since the right hand side is a subspace (show this!) it must be equal to
the span of <span class="math inline">S</span>.
</details>
<p>For any two vector spaces <span class="math inline">U</span> and
<span class="math inline">W</span> we can define the <em>direct sum</em>
<span class="math inline">U\oplus W = \{(u, w)\in U\times W\mid u\in U,
w\in W\}</span>. The vector space addition is <span
class="math inline">(u,w) + (u&#39;,w&#39;) = (u + u&#39;, w +
w&#39;)</span>, <span class="math inline">u,u&#39;\in U</span>, <span
class="math inline">w,w&#39;\in W</span>, and scalar product is <span
class="math inline">a(u,w) = (au,aw)</span>, <span
class="math inline">a\in\boldsymbol{F}</span>.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">U\oplus
W</span> is a vector space</em>.</p>
<p>If <span class="math inline">U\subseteq V</span> is a subspace and
<span class="math inline">W\subseteq V</span> with <span
class="math inline">U + W = V</span> and <span class="math inline">U\cap
V = \{\boldsymbol{0}\}</span> we say <span class="math inline">W</span>
is <em>complementary</em> to <span class="math inline">U</span>. This is
called an <em>internal direct sum</em>.</p>
<p>If <span class="math inline">U\subseteq V</span> is a subspace we can
define the <em>quotient space</em> <span class="math inline">V/U = \{v +
U\mid v\in V\}</span>, where <span class="math inline">v + U = \{v +
u\mid u\in U\}</span>. The quotient space is also a vector space with
addition defined by <span class="math inline">(v + U) + (w + U) = (v +
w) + U</span> and scalar multiplication by <span class="math inline">a(v
+ U) = av + U</span>.</p>
<p><strong>Exercise</strong>. <em>Show the addition and scalar
multiplication are well-defined and <span class="math inline">V/U</span>
is a vector space</em>.</p>
<p><em>Hint</em>: To show addition is well-defined show <span
class="math inline">v + U = v&#39; + U</span> and <span
class="math inline">w + U = w&#39; + U</span> imply <span
class="math inline">(v + w) + U = (v&#39; + w&#39;) + U</span>.</p>
<p>It is true that <span class="math inline">V</span> is isomorphic to
<span class="math inline">U \oplus V/U</span> but that requires the
notion of linear operators.</p>
<h2 id="linear-operators">Linear Operators</h2>
<p>A <em>linear operator</em> from the vector space <span
class="math inline">V</span> to the vector space <span
class="math inline">W</span>, <span class="math inline">T\colon V\to
W</span>, is a function that preserves the vector space structure: <span
class="math inline">T(u + v) = Tu + Tv</span> and <span
class="math inline">T(au) = aTu</span> for <span
class="math inline">u,v\in V</span>, and <span
class="math inline">a\in\boldsymbol{F}</span>.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">T(au +
v) = aTu + v</span>, <span
class="math inline">a\in\boldsymbol{F}</span>, <span
class="math inline">u,v\in V</span> implies <span
class="math inline">T</span> is a linear operator</em>.</p>
<p>The set of all linear operators from a vector space <span
class="math inline">V</span> to a vector space <span
class="math inline">W</span>, <span
class="math inline">\mathcal{L}(V,W)</span>, is also a vector space. The
addition is defined by <span class="math inline">(S + T)v = Sv +
Tv</span>, <span class="math inline">S,T\in\mathcal{L}(V,W)</span>,
<span class="math inline">v\in V</span> and scalar multiplication by
<span class="math inline">(aT)v = a(Tv)</span>, <span
class="math inline">a\in\boldsymbol{F}</span>.</p>
<p><strong>Exercise</strong>. <em>Show <span
class="math inline">\mathcal{L}(V,W)</span> is a vector space</em>.</p>
<p>The values of a linear transformation on a basis determine the linear
transformation. If <span class="math inline">\{v_i\}</span> is a basis
of <span class="math inline">V</span> then for every <span
class="math inline">v\in V</span> there exist unique <span
class="math inline">x_i\in\boldsymbol{F}</span> with <span
class="math inline">v = \sum_i x_i v_i</span> hence <span
class="math inline">Tv = \sum_i x_i Tv_i</span>. Every vector space can
be identified with <span class="math inline">\boldsymbol{F}^S</span>
given a basis <span class="math inline">S\subseteq V</span>.</p>
<p>If <span class="math inline">\{w_j\}</span> is a basis of <span
class="math inline">W</span> then <span class="math inline">Tv_i =
\sum_j t_{ij }w_j</span> for some <span
class="math inline">t_{ij}\in\boldsymbol{F}</span> so <span
class="math inline">T</span> is represented by a matrix <span
class="math inline">(t_{ij})</span> given bases of <span
class="math inline">V</span> and <span class="math inline">W</span>.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">S\colon
U\to V</span> and <span class="math inline">T\colon V\to W</span> are
linear transformations represented by <span
class="math inline">(s_{ij})</span> and <span
class="math inline">(t_{jk})</span> respectively, then <span
class="math inline">TS\colon U\to W</span> is represented by <span
class="math inline">(\sum_j s_{ij} t_{jk})</span></em>.</p>
<p>Matrix multiplication is composition of linear operators.</p>
<details>
<summary>
Solution
</summary>
Let <span class="math inline">\{u_i\}</span> be a basis of <span
class="math inline">U</span>, <span class="math inline">\{v_j\}</span>
be a basis of <span class="math inline">V</span>, and <span
class="math inline">\{w_k\}</span> be a basis of <span
class="math inline">W</span>. We have <span class="math display">
(TS)u_i = T(\sum_j s_{ij} v_j)
    = \sum_j s_{ij} Tv_j
    = \sum_j s_{ij} \sum_k t_{jk} w_k
    = \sum_k (\sum_j s_{ij} t_{jk}) w_k.
</span>
</details>
<p>Note how working in terms of a basis can be tedious.</p>
<p>Let <span class="math inline">\mathcal{M}_{nm} =
\mathcal{L}(\boldsymbol{F}^n, \boldsymbol{F}^m)</span> be the space of
linear operators from …</p>
<p>Let <span class="math inline">E^{ij}</span> be the matrix with <span
class="math inline">k,l</span> entry <span
class="math inline">\delta_{ik}\delta_{jl}</span>…</p>
<p>If <span class="math inline">(e_j)</span> is a basis and <span
class="math inline">\sigma\colon n\to n</span> is a permutation of <span
class="math inline">(1,2,\ldots,n)</span> then <span
class="math inline">(e_{\sigma(j)})</span> is also a basis.</p>
<p>Similarity …</p>
<p>Composition of linear operators defines a product on <span
class="math inline">\mathcal{L}(V)</span> that is associative, but not
commutative unless <span class="math inline">V</span> is
one-dimensional.</p>
<p><strong>Exercise</strong>. <em>Let <span
class="math inline">e_1,e_2</span> be a basis of <span
class="math inline">V</span> and define <span
class="math inline">T,S\in\mathcal{L}(V)</span> by <span
class="math inline">Te_1 = e_2</span>, <span class="math inline">Te_2 =
\boldsymbol{0}</span> and <span class="math inline">Se_1 =
\boldsymbol{0}</span>, <span class="math inline">Se_2 = e_1</span>. Show
<span class="math inline">TS \not= ST</span></em>.</p>
<details>
<summary>
Solution
</summary>
We have <span class="math inline">TSe_1 = Te_2 = e_1</span>, <span
class="math inline">TSe_2 = T\boldsymbol{0}= \boldsymbol{0}</span> and
<span class="math inline">STe_1 = S\boldsymbol{0}=
\boldsymbol{0}</span>, <span class="math inline">STe_2 = Se_1 =
e_2</span> so <span class="math inline">TS\not= ST</span>.
</details>
<p>A vector space with an associative product is an <em>algebra</em>.
The identity <span class="math inline">I\colon V\to V</span> defined by
by <span class="math inline">Iv = v</span>, <span
class="math inline">v\in V</span>. is a left and right multiplicative
identity for <span class="math inline">\mathcal{L}(V)</span>_.</p>
<details>
<summary>
Solution
</summary>
$TI v = T
</details>
<p>The algebra <span class="math inline">\mathcal{L}(V)</span> is not a
<em>division ring</em> if <span class="math inline">V</span> has
dimension greater than one.</p>
<p><strong>Exercise</strong>. <em>Let <span
class="math inline">e_1,e_2</span> be a basis of <span
class="math inline">V</span> and define <span
class="math inline">T,S\in\mathcal{L}(V)</span> by <span
class="math inline">Te_1 = e_1</span>, <span class="math inline">Te_2 =
\boldsymbol{0}</span> and <span class="math inline">Se_1 =
\boldsymbol{0}</span>, <span class="math inline">Se_2 = e_2</span>. Show
<span class="math inline">TS = ST = \boldsymbol{0}</span></em>.</p>
<details>
<summary>
Solution
</summary>
<span class="math inline">TSe_1 = T\boldsymbol{0}= \boldsymbol{0}=
S\boldsymbol{0}= STe_1</span> and <span class="math inline">TSe_2 = Te_2
= \boldsymbol{0}= S\boldsymbol{0}= STe_2</span>.
</details>
<p>If a linear operator <span class="math inline">T\colon V\to W</span>
is one-to-one and onto then <span class="math inline">T</span> is an
<em>isomorphism</em> and we write <span class="math inline">V\cong
W</span>. <em>One-to-one</em> means <span class="math inline">Tu =
Tv</span> implies <span class="math inline">u = v</span> and
<em>onto</em> means for every <span class="math inline">w\in W</span>
there exists <span class="math inline">v\in V</span> with <span
class="math inline">Tv = w</span>. The inverse of such an operator is
defined by <span class="math inline">T^{-1}w = v</span> if and only if
<span class="math inline">Tv = w</span>, just as for any function.</p>
<p><strong>Exercise</strong>. <em>Show <span
class="math inline">T^{-1}</span> is linear</em>.</p>
<p>Isomorphism is an <em>equivalence relation</em> on vector spaces.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">V\cong
V</span>, <span class="math inline">V\cong W</span> implies <span
class="math inline">W\cong V</span>, and <span
class="math inline">U\cong V</span>, <span class="math inline">V\cong
W</span> imply <span class="math inline">U\cong W</span></em>.</p>
<details>
<summary>
Solution
</summary>
The identity function <span class="math inline">I\colon V\to V</span> is
an isomorphism. If <span class="math inline">T\colon V\to W</span> is an
isomorphism then so is <span class="math inline">T^{-1}\colon W\to
V</span>. If <span class="math inline">S\colon U\to V</span> and <span
class="math inline">T\colon V\to W</span> are isomorphisms then so is
the composition <span class="math inline">TS\colon U\to W</span>.
</details>
<p>The fundamental theorem of linear algebra is that two vector spaces
are isomorphic if and only if they have the same dimension. The
non-trivial proof of this is omitted.</p>
<p>Vector spaces are classified up to isomorphism by their dimension.
Contrast this with, e.g., the classification of finite simple
groups.</p>
<p>Although <span class="math inline">\mathcal{L}(V)</span> is not a
division ring we have <span class="math inline">TS</span> is not
invertible if and only if <span class="math inline">T</span> or <span
class="math inline">S</span> is not invertible when <span
class="math inline">V</span> is finite dimensional.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">V</span>
is finite dimensional then <span
class="math inline">T\in\mathcal{L}(V)</span> is not invertible if and
only if there exists <span class="math inline">v\in V</span> with <span
class="math inline">Tv = \boldsymbol{0}</span></em>.</p>
<p><strong>Exercise</strong>. <em>If <span
class="math inline">T,S\in\mathcal{L}(V)</span> where <span
class="math inline">V</span> is finite dimensional then <span
class="math inline">TS\in\mathcal{L}(V)</span> is not invertible if and
only if <span class="math inline">T</span> or <span
class="math inline">S</span> is not invertible</em>.</p>
<h3 id="invariant-subspace">Invariant Subspace</h3>
<p>If <span class="math inline">T\colon V\to V</span> is a linear
operator and <span class="math inline">U</span> is a subspace of <span
class="math inline">V</span> then it is <em>invariant</em> under <span
class="math inline">T</span> if <span class="math inline">TU\subseteq
U</span>. Clearly, <span class="math inline">\{\boldsymbol{0}\}</span>
and <span class="math inline">V</span> are invariant subspaces.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">U</span>
and <span class="math inline">W</span> are invariant subspaces then so
are <span class="math inline">U\cap W</span> and <span
class="math inline">U + W</span></em>.</p>
<p>The <em>kernel</em> of a linear transformation <span
class="math inline">T\colon V\to W</span> is <span
class="math inline">\operatorname{ker}T = \{v\in V\mid Tv = 0\}</span>
and the <em>range</em> is <span class="math inline">\operatorname{ran}T
= \{Tv\mid v\in V\} \subseteq W</span>.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">T\colon
V\to W</span> is a linear operator show the kernel is a subspace of
<span class="math inline">V</span> and the range is a subspace of <span
class="math inline">W</span></em>.</p>
<p><strong>Exercise</strong>. <em>Show <span
class="math inline">\operatorname{ker}T = \{\boldsymbol{0}\}</span>
implies <span class="math inline">T</span> is one-to-one</em>.</p>
<p><em>Hint</em>: Show <span class="math inline">Tu = Tv</span> imples
<span class="math inline">u = v</span>, <span class="math inline">u,v\in
V</span>.</p>
<p>If <span class="math inline">T\colon V\to W</span> is one-to-one we
can define the inverse <span
class="math inline">T^{-1}\colon\operatorname{ran}T\to V</span> by <span
class="math inline">T^{-1}w = v</span> if and only if <span
class="math inline">w = Tv</span>.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">T\colon
V\to V</span> is a linear operator then the kernel and range are
invariant under <span class="math inline">T</span></em>.</p>
<details>
<summary>
Solution
</summary>
We have <span class="math inline">T(\operatorname{ker}T) =
\{\boldsymbol{0}\}\subseteq\operatorname{ker}T</span> and <span
class="math inline">T(\operatorname{ran}T) = T(TV)\subseteq TV =
\operatorname{ran}T</span>.
</details>
<p>If <span class="math inline">v_1, \ldots, v_n</span> is a basis for
<span class="math inline">V</span> we can define a linear operator <span
class="math inline">T\colon V\to \boldsymbol{F}^n</span> by <span
class="math inline">Tv_i = e_i</span> where <span
class="math inline">\{e_i\}</span> is the standard basis of <span
class="math inline">\boldsymbol{F}^n</span>. By linearity <span
class="math inline">T(\sum_i a_i v_i) = \sum_i a_i e_i =
(a_1,\ldots,a_n)\in\boldsymbol{F}^n</span>.</p>
<p><strong>Exercise</strong> <em>Show <span class="math inline">T</span>
is an isomorphism</em>.</p>
<h3 id="eigenvectorsvalues">Eigenvectors/values</h3>
<p>If <span class="math inline">T\colon V\to V</span> is a linear
operator and <span class="math inline">\boldsymbol{F}v</span> is
invariant under <span class="math inline">T</span> for some <span
class="math inline">v\not=\boldsymbol{0}</span> then <span
class="math inline">v</span> is an <em>eigenvector</em> of <span
class="math inline">T</span>. The number <span
class="math inline">\lambda\in\boldsymbol{F}</span> with <span
class="math inline">Tv = \lambda v</span> is the <em>eigenvalue</em>
corresponding to <span class="math inline">v</span>. If <span
class="math inline">v</span> is an eigenvector then <span
class="math inline">av</span> is an eigenvector for all nonzero <span
class="math inline">a\in\boldsymbol{F}</span>.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">v</span>
and <span class="math inline">w</span> are eigenvectors having the same
eigenvalue then <span class="math inline">v + w</span> is an eigenvector
with the same eigenvalue</em>.</p>
<p><em>Hint</em>: <span class="math inline">\operatorname{ker}(T -
\lambda I)</span> is a subspace.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">v</span>
and <span class="math inline">w</span> are eigenvectors with different
eigenvalues then <span class="math inline">v</span> and <span
class="math inline">w</span> are independent</em>.</p>
<details>
<summary>
Solution
</summary>
Suppose <span class="math inline">Tv = \lambda v</span> and <span
class="math inline">Tw = \mu w</span> with <span
class="math inline">\lambda\not=\mu</span>. If <span
class="math inline">av + bw = \boldsymbol{0}</span> then <span
class="math inline">\boldsymbol{0}= (T - \lambda I)(av + bw) = b (\mu -
\lambda)w</span> so <span class="math inline">b = 0</span>. Applying
<span class="math inline">T - \mu I</span> shows <span
class="math inline">a = 0</span>.
</details>
<p>Note if <span class="math inline">Tv = \lambda v</span> then <span
class="math inline">v\in\operatorname{ker}T - \lambda I\neq \{0\}</span>
and so <span class="math inline">T - \lambda I</span> is not invertible.
The <em>spectrum</em> of an operator is the set <span
class="math inline">\sigma(T) = \{\lambda\in\boldsymbol{F}\mid T -
\lambda I\text{ is not invertable}\}</span>. In finite dimensions it is
equal to the set of eigenvalus.</p>
<p>It is not the case every linear operator on a vector space over the
real numbers has an eigenvector, e.g., a rotation about the origin. It
is the case every linear operator on a finite dimensional vector space
over the complex numbers does, but proving that requires more
machinery.</p>
<p><strong>Exercise</strong>. <em>Let <span
class="math inline">T\colon\boldsymbol{C}^2\to\boldsymbol{C}^2</span> be
defined by <span class="math inline">Te_1 = e_2</span> and <span
class="math inline">Te_2 = -e_1</span>. Show <span
class="math inline">\sigma(T) = \{i, -i\}</span> and find the
corresponding eigenvectors</em>.</p>
<details>
<summary>
Solution
</summary>
If <span class="math inline">Tv = \lambda T</span> where <span
class="math inline">v = ae_1 + be_2</span> then <span
class="math inline">Tv = ae_2 - be_1</span> so <span
class="math inline">\lambda a = -b</span> and <span
class="math inline">\lambda b = a</span>. Not both <span
class="math inline">a</span> and <span class="math inline">b</span> are
zero. If <span class="math inline">a\not=0</span> we have <span
class="math inline">a = \lambda b = -\lambda^2 a</span> so <span
class="math inline">\lambda = \pm i</span>. If <span
class="math inline">b\not=0</span> then <span class="math inline">b =
-\lambda a = -\lambda^2 a</span> fo <span class="math inline">\lambda =
\pm i</span>. If <span class="math inline">T(ae_1 + be_2) = i(ae_1 +
be_2)</span> then <span class="math inline">-a = ib</span> and <span
class="math inline">b = ia</span> so <span class="math inline">T(e_1 -
ie_2) = e_2 + ie_1 = i(e_1 - ie_2)</span> hence <span
class="math inline">e_1 - ie_2</span> is an eigenvector with eigenvalue
<span class="math inline">i</span>. Since <span
class="math inline">T(e_1 + ie_2) = e_2 - ie_1 = -i(e_1 + ie_2)</span>
we have <span class="math inline">e_1 + ie_2</span> is an eigenvector
with eigenvalue <span class="math inline">-i</span>.
</details>
<p>If <span class="math inline">V =
\boldsymbol{F}^{\boldsymbol{N}}</span> where <span
class="math inline">\boldsymbol{N}= \{0, 1, 2, \ldots\}</span> are the
natural numbers define the <em>forward shift operator</em> <span
class="math inline">S\colon V\to V</span> by <span
class="math inline">S(v_0, v_1, v_2, \ldots) = (0, v_0, v_1,
\ldots)</span>.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">Sv =
\lambda v</span> then <span class="math inline">v =
\boldsymbol{0}</span></em>.</p>
<details>
<summary>
Solution
</summary>
Note <span class="math inline">S</span> is one-to-one so <span
class="math inline">Sv = 0</span> implies <span class="math inline">v =
0</span> and we can assume <span
class="math inline">\lambda\not=0</span>. If <span
class="math inline">Sv = \lambda v</span> then <span
class="math inline">(0, v_0, v_1,\dots) = (\lambda v_0, \lambda v_1,
\lambda v_2,\ldots)</span>. This implies <span class="math inline">0 =
\lambda v_0</span> so <span class="math inline">v_0 = 0</span>. Likewise
<span class="math inline">0 = v_0 = \lambda v_1</span> so <span
class="math inline">v_1 = 0</span>. By induction <span
class="math inline">v_j = 0</span> for all <span
class="math inline">j</span> so <span class="math inline">v =
\boldsymbol{0}</span>.
</details>
<p>This shows <span class="math inline">S</span> has no
eigenvectors.</p>
<p><strong>Exercise</strong>. <em>Show <span
class="math inline">\sigma(S) = \{\lambda\in\boldsymbol{F}\mid |\lambda|
\le 1\}</span></em>.</p>
<p><em>Hint</em>: If <span class="math inline">|\lambda| &gt; 1</span>
then <span class="math inline">(\lambda I - S)^{-1} = I/\lambda +
S/\lambda^2 + S^2/\lambda^3 + \cdots</span>.</p>
<p>The forward shift operator has lots of invariant subspaces.</p>
<p><strong>Exercise</strong>. <em>Show <span
class="math inline">\mathcal{M}_n = \{v\in V\mid v_j = 0, 0\le j \le
n\}</span> is an invariant subspace for <span class="math inline">n\ge
0</span></em>.</p>
<p>The structure of linear operators on finite dimensional spaces begins
with finding invariant subspaces associated with each eigenvalue. If
<span class="math inline">\sigma(T) =
\{\lambda_1,\ldots,\lambda_m\}</span> and <span
class="math inline">V_i\subseteq V</span> are invariant subspaces with
<span class="math inline">\sigma(T|_{V_i}) = \{\lambda_i\}</span>, <span
class="math inline">1\le i\le m</span> then <span
class="math inline">V_i\cap V_j = \{\boldsymbol{0}\}</span> if <span
class="math inline">i\not=j</span>.</p>
<h3 id="functional-calculus">Functional Calculus</h3>
<p>If <span class="math inline">T\colon V\to V</span> is a linear
operator then so is <span class="math inline">p(T)\colon V\to V</span>
for any polynomial <span class="math inline">p</span>. If <span
class="math inline">q</span> is a polynomial having no roots in the
spectrum of <span class="math inline">T</span> then <span
class="math inline">q(T)^{-1}</span> is also a well-defined linear
operator. This provides a <em>functional calculus</em> from rational
function with no poles in the spectrum of <span
class="math inline">T</span> to linear operators on <span
class="math inline">V</span> <span class="math display">
\Phi\colon\mathcal{R}(\sigma(T))\to \mathcal{L}(V), p/q\mapsto
p(T)q(T)^{-1}.
</span> This is not only a linear map, it also preserves products.</p>
<p><strong>Exercise</strong>. <em>Show <span class="math inline">(rs)(T)
= r(T)s(T)</span> if <span class="math inline">r</span> and <span
class="math inline">s</span> are rational functions with no poles in
<span class="math inline">\sigma(T)</span></em>.</p>
<p><strong>Theorem</strong>. (Spectral mapping theorem) <em>If <span
class="math inline">r\in\mathcal{R}(\sigma(T))</span> then <span
class="math inline">r(\sigma(T)) = \sigma(r(T))</span></em>.</p>
<p><em>Proof</em>: For any <span
class="math inline">\lambda\in\boldsymbol{C}</span> <span
class="math inline">p(z) - p(\lambda) = (z - \lambda)q(z)</span> for
some polynomial <span class="math inline">q</span>. If <span
class="math inline">\lambda\in\sigma(T)</span> then <span
class="math inline">P(T) - p(\lambda)I = (T - \lambda I)q(T)</span>.</p>
<p>We can also define the shift operator <span class="math inline">J =
J^n</span> on <span class="math inline">\boldsymbol{F}^n</span> by <span
class="math inline">J(x_1,\ldots,x_n) = (0,
x_1,\ldots,x_{n-1})</span>.</p>
<p><strong>Exercise</strong>. <em>Show <span
class="math inline">e_n</span> is the only eigenvector and it has
eigenvalue <span class="math inline">0</span></em>.</p>
<p>Note <span class="math inline">J^2(x_1,\ldots,x_n) = (0, 0,
x_1,\ldots,x_{n-2})</span> has eigenvectors <span
class="math inline">e_{n-1}</span> and <span
class="math inline">e_n</span>. Likewise, <span
class="math inline">J^k</span> has eigenvectors <span
class="math inline">e_{n-k+1},\ldots,e_n</span>, <span
class="math inline">1\le k\le n</span>. Clearly <span
class="math inline">J^n = \boldsymbol{0}</span>, the zero operator.</p>
<p>It is not hard to show <span class="math inline">\sigma(J) = 0</span>
but we can use the <em>spectal mapping theorem</em> to give a simple
proof. If <span class="math inline">p</span> is a polynomial and <span
class="math inline">T\colon V\to V</span> is a linear operator then
<span class="math inline">p(T)\colon V\to V</span> can be defined.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">T\colon
V\to V</span> and <span class="math inline">T^m = 0</span> for some
<span class="math inline">m</span> then <span
class="math inline">\sigma(T) = \{0\}</span></em>.</p>
<details>
<summary>
Solution
</summary>
Using the spectral mapping theorem we have <span
class="math inline">\{0\} = \sigma(T^m) = \sigma(T)^m</span>. If <span
class="math inline">0 = \lambda^m</span> then <span
class="math inline">\lambda = 0</span>.
</details>
<h3 id="jordan-canonical-form">Jordan Canonical Form</h3>
<p>Suppose <span class="math inline">T\colon V\to V</span> is a linear
operator on an <span class="math inline">n</span>-dimensional space
<span class="math inline">V</span>. For <span class="math inline">v\in
V</span> define its <em>order</em>, <span
class="math inline">o(v)</span>, to be the minimum <span
class="math inline">m</span> such that <span class="math inline">v, Tv,
\ldots T^mv</span> are linearly dependent. If <span
class="math inline">o(v)</span> equals the dimension of <span
class="math inline">V</span> then <span class="math inline">v</span> is
a <em>cyclic vector</em> for <span class="math inline">T</span> and
<span class="math inline">T^n v = \sum_{0\le j&lt; n}a_j T^jv</span> for
some <span class="math inline">a_j\in\boldsymbol{C}</span>. Using the
basis <span class="math inline">v</span>, <span
class="math inline">Tv</span>, , <span
class="math inline">T^{n-1}v</span> gives a representation for <span
class="math inline">T</span> as a matrix.</p>
<p><span class="math inline">T(\sum_j a_j T_j) = \sum_j
a_jT^{j+1}</span></p>
<p>and <span class="math inline">\sigma(T) = \{0\}</span>. For any <span
class="math inline">v\in V</span> the vectors <span
class="math inline">v</span>, <span class="math inline">Tv</span>, ,
<span class="math inline">T^n</span> are linearly dependent so <span
class="math inline">p(T)v = 0</span> from some polynomial <span
class="math inline">p</span>.</p>
<p>Given <span class="math inline">v_1,\ldots,v_n\in V</span> define the
<em>shift operator</em> <span class="math inline">J\colon V\to V</span>
by <span class="math inline">Jv_i = v_{i+1}</span>, <span
class="math inline">1\le j &lt; n</span> and <span
class="math inline">Jv_n = \boldsymbol{0}</span>.</p>
<h2 id="dual">Dual</h2>
<p>The <em>dual</em> of a vector space <span
class="math inline">V</span> is the set of all linear operators from the
vector space to the underlying field <span class="math inline">V^* =
\mathcal{L}(V,\boldsymbol{F})</span>. The <em>dual pairing</em> <span
class="math inline">\langle .,.\rangle\colon V\times
V^*\to\boldsymbol{F}</span> is defined by <span
class="math inline">\langle v,v^*\rangle = v^*(v)</span>, <span
class="math inline">v\in V</span>, <span class="math inline">v^*\in
V*</span>.</p>
<p>Given any linear operator <span class="math inline">T\colon V\to
W</span> define its <em>adjoint</em> <span class="math inline">T^*\colon
W^*\to V^*</span> by <span class="math inline">\langle v, T^*w^*\rangle
= \langle Tv, w^*\rangle</span>, <span class="math inline">w^*\in
W</span>, <span class="math inline">v\in V</span>. This defines <span
class="math inline">T^*w^*\in V^*</span> at each point <span
class="math inline">v\in V</span>. We also have <span
class="math inline">T^{**}\colon V^{**}\to W^{**}</span>.</p>
<p>Define <span class="math inline">\iota V\to V^{**}</span> by <span
class="math inline">\langle v^*, \iota v\rangle = \langle v,
v^*\rangle</span>.</p>
<p><strong>Exercise</strong>. <em>If <span class="math inline">I\colon
V\to V</span> is the identity, show <span
class="math inline">I^{**}\colon V^{**}\to V^{**}</span> is
one-to-one</em>.</p>
</body>
</html>
